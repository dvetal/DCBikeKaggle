library("kernlab")
install.packages("kernlab")
library("kernlab")
data("iris")
irismodel <- ksvm(Species ~ ., data=iris)
irismodel <- ksvm(Species ~ ., data=iris,
type = "C-bsvc", kernal = "rbfdot",
kpar = list(sigma = 0.1), C=10,
prob.model = TRUE)
head(iris)
irismodel
predict(irismodel, iris[c(3,10,56,68,107,120),-5], type = "probabilities")
predict(irismodel, iris[c(3,10,56,68,107,120),-5], type = "decision")
k <- function(x,y) {}
k <- function(x,y) {
(sum(x*y) +1)*exp(0.001*sum((x - y)^2))
}
class(k) <- "kernal"
data("promotergene")
gene <- ksnm(Class ~ ., data = promotergene, )
gene <- ksnm(Class ~ ., data = promotergene,
kernal = k, C=10, cross =5)
gene <- ksnm(Class ~ ., data = promotergene,
gene <- ksvm(Class ~ ., data = promotergene,
kernal = k, C=10, cross =5)
c
gene <- ksvm(Class ~ ., data = promotergene,
kernal = k, C = 10, cross - 5)
gene <- ksvm(Class ~ ., data = promotergene, kernal = k, C=10, cross = 5)
gene
x <- rbind(matrix(rnorm(120, , 2), matrix(rnorm(120,mean = 3), , 2)))
y <- matrix(c(rep(1,60), rep(-1,60)))
svp <- ksvm(x,y, type = "C-svc", kernal "rbfdot", kpar = list(sigma = 2))
svp <- ksvm(x,y, type = "C-svc", kernal = "rbfdot", kpar = list(sigma = 2))
plot(svp)
x <- rbind(matrix(rnorm(120), , 2), matrix(rnorm(120,mean = 3), , 2))
plot(svp)
svp <- ksvm(x,y, type = "C-svc", kernal = "rbfdot", kpar = list(sigma = 2))
plot(svp)
stopifnot(FALSE)
library(tm)
library(ggplot2)
spam.path <- "data/spam/"
spam2.path <- "data/spam_2/"
easyham.path <- "data/easy_ham/"
easyham2.path <- "data/easy_ham_2/"
hardham.path <- "data/hard_ham/"
hardham2.path <- "data/hard_ham2/"
#Open an individual file for reading text (rt)
#read each line into a vector "text"
#get onlye the body of the messages
# close the message by collapsing all of the body lines
#into a single text element for the etire message
#close
get.msg <- function(path) {
con <- file(path, open="rt", encoding="latin1")
text <- readLines(con)
#The message always begins after the first
#full line break
msg <- text[seq(which(text=="")[1]+1,length(text),1)]
close(con)
return(paste(msg, collapse = "\n"))
}
#create an array of all of the documen bodies.
#and get only the spam
spam.docs <-dir(spam.path)
#get only the acutal docs, not "cmds"
spam.docs <- spam.docs[which(spam.docs!="cmds")]
all.spam <- sapply(spam.docs, function(p) get.msg(paste(spam.path,p,sep="")))
#do the same as above for easyham
easyham.docs <-dir(easyham.path)
#get only the acutal docs, not "cmds"
easyham.docs <- easyham.docs[which(spam.docs!="cmds")]
all.easyham <- sapply(easyham.docs[1:length(spam.docs)], function(p) get.msg(paste(easyham.path,p,sep="")))
get.tdm <- function(doc.vec) {
doc.corpus <- Corpus(VectorSource(doc.vec))
control <- list(stopwords=T, removePunctuation=T,removeNumbers=T
,minDocFreq=2)
doc.dtm <- TermDocumentMatrix(doc.corpus,control)
return(doc.dtm)
}
#below creates the spam.df with occurrence ,density, frequency of words
spam.tdm <- get.tdm(all.spam)
spam.matrix <- as.matrix(spam.tdm)
spam.counts <- rowSums(spam.matrix)
spam.df <- data.frame(cbind(names(spam.counts),
as.numeric(spam.counts)),stringsAsFactors=F)
names(spam.df) <- c("term","frequency")
spam.df$frequency <- as.numeric(spam.df$frequency)
spam.occurrence <- sapply(1:nrow(spam.matrix),
function(i) {length(which(spam.matrix[i,] >0))/ncol(spam.matrix)})
spam.density <- spam.df$frequency/sum(spam.df$frequency)
spam.df <- transform(spam.df, density=spam.density, occurrence=spam.occurrence)
#now the same for easy ham documents but only the first 500
#as we want our assumption that a spam doc is equally likely
#to occur as a ham doc.  We only have 500 spam docs...
easyham.tdm <- get.tdm(all.easyham)
easyham.matrix <- as.matrix(easyham.tdm)
easyham.counts <- rowSums(easyham.matrix)
easyham.df <- data.frame(cbind(names(easyham.counts),
as.numeric(easyham.counts)),stringsAsFactors=F)
names(easyham.df) <- c("term","frequency")
easyham.df$frequency <- as.numeric(easyham.df$frequency)
easyham.occurrence <- sapply(1:nrow(easyham.matrix),
function(i) {length(which(easyham.matrix[i,] >0))/ncol(easyham.matrix)})
easyham.density <- easyham.df$frequency/sum(easyham.df$frequency)
easyham.df <- transform(easyham.df, density=easyham.density, occurrence=easyham.occurrence)
#create the predictor function called classify.email
classify.email <- function(path, training.df, prior=0.5, c=1e-6) {
msg <- get.msg(path)
msg.tdm <- get.tdm(msg)
msg.freq <- rowSums(as.matrix(msg.tdm))
#find intersections of words in test with training set
msg.match <- intersect(names(msg.freq), training.df$term)
if(length(msg.match) < 1) {
return(prior*c^(length(msg.freq)))
}
else {
match.probs <- training.df$occurrence[match(msg.match, training.df$term)]
return(prior * prod(match.probs) * c^(length(msg.freq)-length(msg.match)))
}
}
library(tm)
library(ggplot2)
spam.path <- "data/spam/"
spam2.path <- "data/spam_2/"
easyham.path <- "data/easy_ham/"
easyham2.path <- "data/easy_ham_2/"
hardham.path <- "data/hard_ham/"
hardham2.path <- "data/hard_ham2/"
#Open an individual file for reading text (rt)
#read each line into a vector "text"
#get onlye the body of the messages
# close the message by collapsing all of the body lines
#into a single text element for the etire message
#close
get.msg <- function(path) {
con <- file(path, open="rt", encoding="latin1")
text <- readLines(con)
#The message always begins after the first
#full line break
msg <- text[seq(which(text=="")[1]+1,length(text),1)]
close(con)
return(paste(msg, collapse = "\n"))
}
#create an array of all of the documen bodies.
#and get only the spam
spam.docs <-dir(spam.path)
#get only the acutal docs, not "cmds"
spam.docs <- spam.docs[which(spam.docs!="cmds")]
all.spam <- sapply(spam.docs, function(p) get.msg(paste(spam.path,p,sep="")))
#do the same as above for easyham
easyham.docs <-dir(easyham.path)
#get only the acutal docs, not "cmds"
easyham.docs <- easyham.docs[which(spam.docs!="cmds")]
all.easyham <- sapply(easyham.docs[1:length(spam.docs)], function(p) get.msg(paste(easyham.path,p,sep="")))
get.tdm <- function(doc.vec) {
doc.corpus <- Corpus(VectorSource(doc.vec))
control <- list(stopwords=T, removePunctuation=T,removeNumbers=T
,minDocFreq=2)
doc.dtm <- TermDocumentMatrix(doc.corpus,control)
return(doc.dtm)
}
#below creates the spam.df with occurrence ,density, frequency of words
spam.tdm <- get.tdm(all.spam)
spam.matrix <- as.matrix(spam.tdm)
spam.counts <- rowSums(spam.matrix)
spam.df <- data.frame(cbind(names(spam.counts),
as.numeric(spam.counts)),stringsAsFactors=F)
names(spam.df) <- c("term","frequency")
spam.df$frequency <- as.numeric(spam.df$frequency)
spam.occurrence <- sapply(1:nrow(spam.matrix),
function(i) {length(which(spam.matrix[i,] >0))/ncol(spam.matrix)})
spam.density <- spam.df$frequency/sum(spam.df$frequency)
spam.df <- transform(spam.df, density=spam.density, occurrence=spam.occurrence)
#now the same for easy ham documents but only the first 500
#as we want our assumption that a spam doc is equally likely
#to occur as a ham doc.  We only have 500 spam docs...
easyham.tdm <- get.tdm(all.easyham)
easyham.matrix <- as.matrix(easyham.tdm)
easyham.counts <- rowSums(easyham.matrix)
easyham.df <- data.frame(cbind(names(easyham.counts),
as.numeric(easyham.counts)),stringsAsFactors=F)
names(easyham.df) <- c("term","frequency")
easyham.df$frequency <- as.numeric(easyham.df$frequency)
easyham.occurrence <- sapply(1:nrow(easyham.matrix),
function(i) {length(which(easyham.matrix[i,] >0))/ncol(easyham.matrix)})
easyham.density <- easyham.df$frequency/sum(easyham.df$frequency)
easyham.df <- transform(easyham.df, density=easyham.density, occurrence=easyham.occurrence)
#create the predictor function called classify.email
classify.email <- function(path, training.df, prior=0.5, c=1e-6) {
msg <- get.msg(path)
msg.tdm <- get.tdm(msg)
msg.freq <- rowSums(as.matrix(msg.tdm))
#find intersections of words in test with training set
msg.match <- intersect(names(msg.freq), training.df$term)
if(length(msg.match) < 1) {
return(prior*c^(length(msg.freq)))
}
else {
match.probs <- training.df$occurrence[match(msg.match, training.df$term)]
return(prior * prod(match.probs) * c^(length(msg.freq)-length(msg.match)))
}
}
getwd()
setwd("projects/ML_for_Hackers/03-Classification")
getwd()
library(tm)
library(ggplot2)
spam.path <- "data/spam/"
spam2.path <- "data/spam_2/"
easyham.path <- "data/easy_ham/"
easyham2.path <- "data/easy_ham_2/"
hardham.path <- "data/hard_ham/"
hardham2.path <- "data/hard_ham2/"
#Open an individual file for reading text (rt)
#read each line into a vector "text"
#get only the body of the messages
# close the message by collapsing all of the body lines
#into a single text element for the etire message
#close
get.msg <- function(path) {
con <- file(path, open="rt", encoding="latin1")
text <- readLines(con)
#The message always begins after the first
#full line break
msg <- text[seq(which(text=="")[1]+1,length(text),1)]
close(con)
return(paste(msg, collapse = "\n"))
}
#create an array of all of the documen bodies.
#and get only the spam
spam.docs <-dir(spam.path)
#get only the acutal docs, not "cmds"
spam.docs <- spam.docs[which(spam.docs!="cmds")]
all.spam <- sapply(spam.docs, function(p) get.msg(paste(spam.path,p,sep="")))
#do the same as above for easyham
easyham.docs <-dir(easyham.path)
#get only the acutal docs, not "cmds"
easyham.docs <- easyham.docs[which(spam.docs!="cmds")]
all.easyham <- sapply(easyham.docs[1:length(spam.docs)], function(p) get.msg(paste(easyham.path,p,sep="")))
get.tdm <- function(doc.vec) {
doc.corpus <- Corpus(VectorSource(doc.vec))
control <- list(stopwords=T, removePunctuation=T,removeNumbers=T
,minDocFreq=2)
doc.dtm <- TermDocumentMatrix(doc.corpus,control)
return(doc.dtm)
}
#below creates the spam.df with occurrence ,density, frequency of words
spam.tdm <- get.tdm(all.spam)
spam.matrix <- as.matrix(spam.tdm)
spam.counts <- rowSums(spam.matrix)
spam.df <- data.frame(cbind(names(spam.counts),
as.numeric(spam.counts)),stringsAsFactors=F)
names(spam.df) <- c("term","frequency")
spam.df$frequency <- as.numeric(spam.df$frequency)
spam.occurrence <- sapply(1:nrow(spam.matrix),
function(i) {length(which(spam.matrix[i,] >0))/ncol(spam.matrix)})
spam.density <- spam.df$frequency/sum(spam.df$frequency)
spam.df <- transform(spam.df, density=spam.density, occurrence=spam.occurrence)
#now the same for easy ham documents but only the first 500
#as we want our assumption that a spam doc is equally likely
#to occur as a ham doc.  We only have 500 spam docs...
easyham.tdm <- get.tdm(all.easyham)
easyham.matrix <- as.matrix(easyham.tdm)
easyham.counts <- rowSums(easyham.matrix)
easyham.df <- data.frame(cbind(names(easyham.counts),
as.numeric(easyham.counts)),stringsAsFactors=F)
names(easyham.df) <- c("term","frequency")
easyham.df$frequency <- as.numeric(easyham.df$frequency)
easyham.occurrence <- sapply(1:nrow(easyham.matrix),
function(i) {length(which(easyham.matrix[i,] >0))/ncol(easyham.matrix)})
easyham.density <- easyham.df$frequency/sum(easyham.df$frequency)
easyham.df <- transform(easyham.df, density=easyham.density, occurrence=easyham.occurrence)
#create the predictor function called classify.email
classify.email <- function(path, training.df, prior=0.5, c=1e-6) {
msg <- get.msg(path)
msg.tdm <- get.tdm(msg)
msg.freq <- rowSums(as.matrix(msg.tdm))
#find intersections of words in test with training set
msg.match <- intersect(names(msg.freq), training.df$term)
if(length(msg.match) < 1) {
return(prior*c^(length(msg.freq)))
}
else {
match.probs <- training.df$occurrence[match(msg.match, training.df$term)]
return(prior * prod(match.probs) * c^(length(msg.freq)-length(msg.match)))
}
}
hardham.docs <- dir(hardham.path)
hardham.docs
hardham.docs <- hardham.docs[which(hardham.docs != "cmds")]
hardham.docs
training.df=spam.df))
hardham.spamtest <- sapply(hardham.docs,
function(p) classify.email(paste(hardham.path,p,sep=""),
training.df=spam.df))
hardham.hamtest <- sapply(hardham.docs,
function(p) classify.email(paste(hardham.path,p,sep=""),
training.df=easyham.df))
hardham.res <- ifelse(hardham.spamtest > hardham.hamtest, T, F)
summary(hardham.res)
getwd()
setwd("../..")
getwd()
setwd("DCBike")
#This is my code to compete on the DC Bike Share Kaggle Competition.
#Load Libraries
library(ggplot2)
#Load Raw Data
RawTrain <- read.csv("data/train.csv",header=T)
RawTest  <- read.csv("data/test.csv",header=T)
RawSubmission  <- read.csv("data/sampleSubmission.csv",header=T)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count,color=datetime)) + geom_point()
reg.plot + facet_grid( ~ weather)
names(RawTrain)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count,color=weather)) + geom_point()
reg.plot + facet_grid( ~ weather)
qplot(RawTest, weather, geom="histogram")
qplot(RawTest, data=weather, geom="histogram")
names(RawTest)
qplot(RawTest, data=RawTest$weather, geom="histogram")
qplot(RawTest, data=RawTest$weather, geom="histogram",binwidth=1)
reg.plot <- ggplot(RawTest, aes(x=weather)) + geom_histogram()
reg.plot <- ggplot(RawTest, aes(x=weather)) + geom_histogram()
reg.plot
qplot(RawTest$, data=RawTest$weather, geom="histogram",binwidth=5)
qplot(RawTest$, data=RawTest$weather, geom="histogram",binwidth=1)
qplot(RawTest, data=RawTest$weather, geom="histogram",binwidth=1)
qplot(RawTest$weather, data=RawTest$weather, geom="histogram",binwidth=1)
reg.plot <- ggplot(RawTest, aes(x=weather)) + geom_histogram()
test.plot <- ggplot(RawTest, aes(x=weather)) + geom_histogram()
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count,color=weather)) + geom_point()
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count,color=weather)) + geom_point(aes(size=0.5))
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count,color=weather)) + geom_point(aes(size=0.05))
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count,color=weather)) + geom_point(aes(size=0.005))
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count,color=weather)) + geom_point(aes(size=5))
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count,color=weather)) + geom_point(aes(size=5))
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count,color=weather,size=5)) + geom_point(aes(size=5))
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count,color=weather,size=0.05)) + geom_point(aes(size=5))
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count)) + geom_point(aes(size=5))
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count)) + geom_point(size=5)
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count)) + geom_point(size=0.05)
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count)) + geom_point(size=1)
reg.plot + facet_grid( ~ weather)
reg.plot <- ggplot(RawTrain, aes(x=weather, y=count)) + geom_point(size=1)
reg.plot + facet_grid( ~ temp)
names(RawTrain)
reg.plot <- ggplot(RawTrain, aes(x=weather, y=count)) + geom_point(size=1)
reg.plot + facet_grid( ~ holiday)
reg.plot <- ggplot(RawTrain, aes(x=weather, y=holiday)) + geom_point(size=1)
reg.plot + facet_grid( ~ holiday)
reg.plot <- ggplot(RawTrain, aes(x=weather, y=holiday)) + geom_point(size=1)
reg.plot <- ggplot(RawTrain, aes(x=weather, y=holiday)) + geom_point(size=1)
reg.plot + facet_grid( ~ casual)
reg.plot <- ggplot(RawTrain, aes(x=weather, y=count)) + geom_point(size=1)
reg.plot + facet_grid( ~ holiday)
reg.plot <- ggplot(RawTrain, aes(x=temp, y=count)) + geom_point(size=1)
reg.plot + facet_grid( ~ casual)
reg.plot <- ggplot(RawTrain, aes(x=season, y=count)) + geom_point(size=1)
reg.plot + facet_grid( ~ casual)
reg.plot
reg.plot + facet_grid( ~ registered)
reg.plot <- ggplot(RawTrain, aes(x=season, y=count)) + geom_point(size=1)
reg.plot
reg.plot + facet_grid(registered ~ )
reg.plot + facet_grid(registered ~ )
reg.plot + facet_grid( registered ~ )
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count)) + geom_point(size=1)
reg.plot + facet_grid( ~ weather)
reg.plot + facet_grid(season ~ weather)
reg.plot + facet_grid(season ~ )
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count)) + geom_point(size=1)
reg.plot + facet_grid(season ~ )
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count)) + geom_point(size=1)
reg.plot + facet_grid(season ~ weather)
summary(RawTrain$weather)
RawTrain$weather[4]
RawTrain$weather[3]
RawTrain$weather[2]
count(RawTrain[weather==4])
sum(RawTrain[weather==4])
RawTrain[weather==4]
RawTrain[RawTrain$weather==4]
RawTrain[which(RawTrain$weather==4)]
which(RawTrain$weather==4)
which(RawTrain$weather==3)
which(RawTrain$weather==4)
RawTrain$weather[which(RawTrain$weather==4)]
RawTrain$weather[which(RawTrain$weather==4)] <- 3
which(RawTrain$weather==4)
names(RawTrain)
count(RawTrain$weather[which(RawTrain$weather==4)])
sum(RawTrain$weather[which(RawTrain$weather==4)])
sum(RawTrain$weather[which(RawTrain$weather==3)])
sum(RawTrain$weather[which(RawTrain$weather==2)])
sum(RawTrain$weather[which(RawTrain$weather==1)])
nrow(RawTrain)
which(RawTrain$weather==2)
RawTrain$weather==2
sum(RawTrain$weather==2)
sum(RawTrain$weather==3)
sum(RawTrain$weather==4)
sum(RawTrain$weather==1)
reg.plot <- ggplot(RawTrain, aes(x=atemp, y=count)) + geom_point(size=1)
reg.plot + facet_grid(season ~ weather)
names(RawTrain)
box.plot <- ggplot(RawTrain,aes(x=registered, y=windspeed, fill=registered)) + geom_boxplot()
box.plot
box.plot <- ggplot(RawTrain,aes(x=registered, y=windspeed, fill=registered)) + geom_boxplot()
box.plot
box.plot <- ggplot(RawTrain,aes(x=registered, y=windspeed)) + geom_boxplot()
box.plot
box.plot <- ggplot(RawTrain,aes(x=casual, y=windspeed)) + geom_boxplot()
box.plot
summary(casual)
summary(RawTrain$casual)
RawTrain(head)
head(RawTrain)
box.plot <- ggplot(RawTrain,aes(x=season, y=windspeed)) + geom_boxplot()
box.plot
str(RawTrain)
toString(RawTrain$weather)
str(RawTrain)
RawTrain$weather <- toString(RawTrain$weather)
str(RawTrain)
RawTrain[1]
RawTrain[2]
RawTrain[2,]
RawTrain <- read.csv("data/train.csv",header=T)
RawTest  <- read.csv("data/test.csv",header=T)
RawSubmission  <- read.csv("data/sampleSubmission.csv",header=T)
